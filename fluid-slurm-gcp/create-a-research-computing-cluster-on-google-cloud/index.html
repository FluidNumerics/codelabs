
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Create a Research Computing Cluster on Google Cloud</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="UA-125720260-6"
                  id="fluid-slurm-gcp/create-a-research-computing-cluster-on-google-cloud"
                  title="Create a Research Computing Cluster on Google Cloud"
                  environment="web"
                  feedback-link="https://forms.gle/q5HaN43HyVXLSLmM9">
    
      <google-codelab-step label="Introduction" duration="0">
        <p class="image-container"><img style="width: 148.50px" src="img/b095befab07633f0.png"></p>
<p><strong>Last Updated:</strong> 2020-07-27</p>
<h2 is-upgraded><strong>Use cases for  Fluid Numerics&#39; HPC Cluster on Google Cloud Platform</strong></h2>
<p>Organizations are beginning to investigate how to leverage cloud computing resources to meet highly variable high performance computing needs that often exceed the capacity of on-premise compute resources. </p>
<p>Google Cloud Platform (GCP) offers burstable access to a staggering amount of compute resources worldwide that includes <a href="https://cloud.google.com/compute/docs/cpu-platforms" target="_blank">CPUs</a>, <a href="https://cloud.google.com/compute/docs/gpus/" target="_blank">GPUs</a>, and TPUs. GCP also offers flexible <a href="https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type" target="_blank">custom system configurations</a>, where you can set a desired core-to-memory ratio for each compute instance. Additionally, GCP provides access to a range of data storage solutions that can attach to compute instances, including<a href="https://cloud.google.com/compute/docs/disks/" target="_blank"> HDD and SSD persistent disks</a>, high bandwidth <a href="https://cloud.google.com/compute/docs/disks/#localssds" target="_blank">local SSDs</a>, <a href="https://cloud.google.com/filestore/" target="_blank">NFS file-systems</a>, and <a href="https://cloud.google.com/storage/" target="_blank">object storage</a>. The partner ecosystem on GCP has also brought <a href="https://console.cloud.google.com/marketplace/details/ddnstorage/cloud-edition-for-lustre" target="_blank">Lustre file-systems</a> to the cloud.</p>
<p>Fluid Numerics has developed a globally scalable and highly customizable HPC cluster on Google Cloud Platform. Fluid-Slurm-GCP is built around the <a href="https://slurm.schedmd.com/" target="_blank">Slurm job scheduler</a> and Google Cloud Platform engineering components. With this deployment, you can easily take advantage of compute resources across Google&#39;s data-centers worldwide, while maintaining a familiar HPC environment for you and your team. The fluid-slurm-gcp cluster can be used as a cloud-native or federated solution that provides cloud-bursting capabilities from your on-premise systems. </p>
<p>Further, with backgrounds in academia and government research, Fluid Numerics understands the requirements for expensing costs for government funded research. We have designed the fluid-slurm-gcp system to permit <a href="https://help.fluidnumerics.com/slurm-gcp/documentation/multi-project-slurm-gcp-for-universities-and-large-organizations" target="_blank">granular billing breakdowns</a> and controls through cost management platforms like <a href="https://www.orbitera.com/" target="_blank">Orbitera</a>.</p>
<h3 is-upgraded><strong>Infrastructure</strong></h3>
<p class="image-container"><img style="width: 624.00px" src="img/6ee7300bc951090c.png"></p>
<p>fluid-slurm-gcp provides a design on cloud resources that is similar in look-and-feel to on-premise HPC infrastructure. The deployment has a number of static login nodes, where users access the cluster, and a <em>controller</em> instance that hosts the Slurm job scheduler. Compute nodes are either static or ephemeral.</p>
<p>Static compute nodes persist as long as the deployment is active and remain ready to receive workloads from Slurm. Ephemeral compute nodes are created on-the-fly, as needed, to meet variable compute capacity demands. When ephemeral nodes become idle, Slurm automatically removes the compute resources from GCP so that you pay only for compute cycles as needed. To develop a better understanding of the costs on GCP for fluid-slurm-gcp, check out our <a href="https://help.fluidnumerics.com/slurm-gcp/pricing" target="_blank">pricing documentation</a>. </p>
<h3 is-upgraded><strong>Software tools</strong></h3>
<h4 is-upgraded>Cluster-Services</h4>
<p><a href="https://help.fluidnumerics.com/slurm-gcp/documentation/cluster-services" target="_blank">Cluster-Services</a>, a command-line tool developed by Fluid Numerics to manage compute partitions and available machines, Slurm accounting configurations and compute partition access for users, and network attached storage mounts. All of the images for fluid-slurm-gcp come with this command-line tool to make managing your globally scalable HPC cluster simpler and cost-effective.</p>
<h4 is-upgraded>Spack</h4>
<p><a href="https://spack.io/" target="_blank">Spack</a> is becoming a well accepted package manager for HPC applications. Spack supports a <a href="https://spack.readthedocs.io/en/latest/package_list.html" target="_blank">large number of HPC packages</a>, while managing all of the package dependencies for you.  Spack offers a straightforward process to <a href="https://spack-tutorial.readthedocs.io/en/latest/tutorial_packaging.html" target="_blank">integrate your HPC software into the Spack ecosystem</a> so that you can share your code with others more easily. To get started with using Spack for HPC package management, check out the <a href="https://spack-tutorial.readthedocs.io/en/latest/tutorial_basics.html#installing-packages" target="_blank">Spack package installation tutorial</a>.</p>
<h4 is-upgraded>Singularity</h4>
<p>HPC developers are beginning to develop strategies for creating portable HPC applications. Containerization has proven to be an excellent solution for the portability problem by allowing developers to package development environments alongside applications and &#34;shipping&#34; the containers to production systems. We opt to offer Singularity as our containerization platform of choice.</p>
<p><a href="https://sylabs.io/singularity/" target="_blank">Singularity</a> is a container platform that was developed specifically for HPC uses. To run HPC container images, users do not require privilege escalation. Further, <a href="https://sylabs.io/guides/3.3/user-guide/mpi.html" target="_blank">Singularity works naturally with MPI applications</a> that leverage resources across multiple compute nodes and even has flags that allow you to <a href="https://sylabs.io/guides/3.5/user-guide/gpu.html" target="_blank">leverage GPU and multi-GPU platforms</a>. </p>
<h2 is-upgraded><strong>What you will build</strong></h2>
<p>In this codelab, you are going to create a cloud-native HPC cluster on Google Cloud Platform and configure high-availability (multi-zone) and globally-scalable (multi-region) compute partitions. </p>
<aside class="warning"><p><strong>Note: </strong><a href="https://cloud.google.com/products/calculator/#id=5df12891-6c50-4d3f-97b9-c483439af101" target="_blank">This tutorial is estimated to cost $2.09 on Google Cloud Platform</a> (actual costs may vary)</p>
</aside>
<h2 is-upgraded><strong>What you will learn</strong></h2>
<ul>
<li>How to configure Identity and Access Management (IAM) policies for operating an HPC cluster on Google Cloud Platform</li>
<li>How to deploy a cloud-native HPC cluster with the Slurm job scheduler</li>
<li>How to set up basic Slurm accounting with Fluid Numerics&#39; cluster-services</li>
<li>How to submit a job step with Slurm</li>
</ul>
<h2 is-upgraded><strong>What you will need</strong></h2>
<ul>
<li><a href="https://gsuite.google.com/" target="_blank">GSuite</a>, <a href="https://cloud.google.com/identity" target="_blank">Cloud Identity</a>, or <a href="https://www.google.com/gmail/" target="_blank">Gmail Account</a></li>
<li><a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects" target="_blank">Google Cloud Platform Project with Billing enabled</a></li>
<li>Project owner role on your GCP Project</li>
<li><a href="https://cloud.google.com/compute/quotas" target="_blank">Sufficient Compute Engine Quota</a> in two GCP regions</li>
<li>Basic Command-Line Linux Experience</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Project, Quota, and Permissions Setup" duration="10">
        <h2 is-upgraded><strong>Project Setup</strong></h2>
<aside class="special"><p><strong>Tip:</strong> If you do not have a GSuite, Cloud Identity, or Gmail account, <a href="https://support.google.com/mail/answer/56256?hl=en" target="_blank">take the time to create a Gmail account</a>. You will use this account for Google Cloud Platform. As a first time user for Google Cloud Platform, you will receive $300 in credits that you can use for this tutorial!</p>
</aside>
<p>This codelab assumes that you already have a GSuite, Cloud Identity, or Gmail account. To get started, sign in to <a href="https://console.cloud.google.com" target="_blank">console.cloud.google.com</a> using your GSuite, Cloud Identity, or Gmail account.</p>
<aside class="special"><p><strong>Tip: </strong>If you don&#39;t have an active project in GCP, <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects" target="_blank">create a project</a> and <a href="https://cloud.google.com/billing/docs/how-to/manage-billing-account#create_a_new_billing_account" target="_blank">enable billing</a> for this project.</p>
</aside>
<iframe class="youtube-video" src="https://www.youtube.com/embed/WXVPGfNBMOk?rel=0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>In the project selection dropdown menu, select the project you want to use to create your HPC cluster. In the products and services menu, navigate to APIs &amp; Services &gt; Dashboard.</p>
<p class="image-container"><img style="width: 316.00px" src="img/6eba7e4957bf17a6.png"></p>
<p>Click on + Enable APIs and Services. Search for &#34;Compute Engine&#34; and click on the Compute Engine API. Click Enable and wait for the Compute Engine API to be configured and setup.</p>
<h2 is-upgraded><strong>Verify GCP Quota</strong></h2>
<p>For this demo, you will need the following minimum quota</p>
<ul>
<li>12 vCPUs in us-west1 and europe-west1.</li>
<li>120 GB PD-Standard Disk in us-west1 and europe-west1</li>
</ul>
<p class="image-container"><img style="width: 369.81px" src="img/b24a27997eda2105.png"></p>
<p>To verify and/or modify your quota, navigate to IAM &amp; Admin &gt; Quotas in the Products and Services menu. Use the following filters to display only CPU and PD-Standard disk quota in us-west1 and europe-west1</p>
<ul>
<li>In the Services dropdown menu, select only &#34;Compute Engine API&#34;. </li>
<li>In the Metrics dropdown menu, select &#34;CPUs&#34; and &#34;Persistent Disk Standard (GB)&#34;</li>
<li>In the Location dropdown menu, select &#34;europe-west1&#34; and &#34;us-west1&#34;</li>
</ul>
<p class="image-container"><img style="width: 650.67px" src="img/e1faf43c9ca90cc4.png"></p>
<p>Your default quota <em>should</em> be sufficient for this tutorial. In the event it is not, check the box next to &#34;Service&#34; and click &#34;Edit Quotas&#34;. Follow the prompts in the info panel that appears on the right side of the screen. You may need to wait for Google&#39;s quota team to update your quota.</p>
<h2 is-upgraded>Set IAM Policies</h2>
<p>In HPC, there are clear distinctions between system administrators and system users. System administrators generally have &#34;root access&#34; enabling them to manage and operate compute resources. System users are generally researchers, scientists, and application engineers that only need to leverage the resources to execute jobs.</p>
<p>On Google Cloud Platform, the <a href="https://cloud.google.com/compute/docs/oslogin/" target="_blank">OS Login API</a> provisions POSIX user information from GSuite, Cloud Identity, and Gmail accounts. Additionally, OS Login integrates with GCP&#39;s <a href="https://cloud.google.com/iam" target="_blank">Identity and Access Management (IAM)</a> system to determine if users should be allowed to escalate privileges on Linux systems.</p>
<p>In this tutorial, we assume you are filling the system administrator and compute engine administrator roles. We will configure IAM policies to give you sufficient permissions to accomplish the following tasks </p>
<ul>
<li>Create/Delete Google Compute Engine (GCE) VM instances</li>
<li>SSH into GCE VM instances</li>
<li>Escalate privileges on GCE VM instances</li>
</ul>
<p class="image-container"><img style="width: 516.50px" src="img/74e3b6764c32a120.png"></p>
<p>To give yourself the necessary IAM roles to complete this tutorial</p>
<ol type="1" start="1">
<li>Navigate to IAM &amp; Admin &gt; IAM in the Products and Services menu. </li>
<li>Click &#34;+Add&#34; near the top of the page. </li>
<li>Type in your GSuite account, Cloud Identity Account, or Gmail account under &#34;Members&#34;</li>
<li>Add the following roles : Project Editor, Compute OS Admin Login, Service Account User</li>
<li>Click Save</li>
</ol>
<aside class="warning"><p><strong>Caution:</strong> Generally, Project Editor provides more permissions than necessary for most users. See <a href="https://help.fluidnumerics.com/slurm-gcp/documentation/tutorials/gcp-iam-role-setup" target="_blank">Fluid Numerics documentation</a> for recommended permissions for system administrators and system users of fluid-slurm-gcp</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Deploy Fluid-Slurm-GCP from the GCP Marketplace" duration="1">
        <p>Now you have verified your quota and permissions and are ready to launch the Fluid-Slurm-GCP HPC cluster on Google Cloud Platform. In these next few steps, you will create your cluster.</p>
<ol type="1" start="1">
<li>Open <a href="https://console.cloud.google.com/marketplace/details/fluid-cluster-ops/fluid-slurm-gcp" target="_blank">https://console.cloud.google.com/marketplace/details/fluid-cluster-ops/fluid-slurm-gcp</a> in a new tab. </li>
</ol>
<aside class="warning"><p><strong>Caution:</strong> Make sure that you are in the same GCP project that you configured in the previous section and using the correct GSuite, Cloud Identity, or Gmail account.</p>
</aside>
<ol type="1" start="2">
<li>Click &#34;Launch&#34;</li>
<li>Set the following parameters, while leaving the others as their default.</li>
</ol>
<table>
<tr><td colspan="1" rowspan="1"><p>Deployment name</p>
</td><td colspan="1" rowspan="1"><p>codelab-demo</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Zone</p>
</td><td colspan="1" rowspan="1"><p>us-west1-b</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Partition Name</p>
</td><td colspan="1" rowspan="1"><p>first-partition</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Static node count</p>
</td><td colspan="1" rowspan="1"><p>0</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Max node count</p>
</td><td colspan="1" rowspan="1"><p>2</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Machine type</p>
</td><td colspan="1" rowspan="1"><p>n1-standard-2</p>
</td></tr>
</table>
<ol type="1" start="4">
<li>Click &#34;Deploy&#34; and wait for the cluster to be created.</li>
</ol>
<p>Congratulations! You have just created the a login node and controller on Google Cloud Platform. These instances can be used to spin-up ephemeral compute nodes to execute HPC workloads.</p>
<aside class="warning"><p><strong>Caution:</strong> Sometimes, the deployment-manager page will show the solution has been created, but the startup-scripts may still be configuring. If you log in to the login node and see that /home, /apps, and /etc have not been mounted, log out and wait for (at most) another minute.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Verify and test the Slurm Cluster" duration="10">
        <iframe class="youtube-video" src="https://www.youtube.com/embed/9fj7ne6cmH0?rel=0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>In this section, we will access the cluster&#39;s login node, verify mount and partition settings, add our account to the default Slurm account, and submit a simple job to verify that the cluster is working.</p>
<h2 is-upgraded>Verify your cluster has been properly set up</h2>
<p class="image-container"><img style="width: 624.00px" src="img/e00bfb83ad63227f.png"></p>
<p>Now that your deployment has been launched, it is time to access the login node. You can do this by clicking the SSH button from the deployment-manager page. This will open up a browser window with an active terminal on the login node.</p>
<p class="image-container"><img style="width: 514.50px" src="img/6bad2664f5634ea3.png"></p>
<p>Once you have logged into the login node, let&#39;s make sure that /home, /apps, and /etc are mounted from the controller.</p>
<pre><code>$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
devtmpfs                            7.3G     0  7.3G   0% /dev
tmpfs                               7.3G     0  7.3G   0% /dev/shm
tmpfs                               7.3G  8.5M  7.3G   1% /run
tmpfs                               7.3G     0  7.3G   0% /sys/fs/cgroup
/dev/sda1                            20G  3.1G   17G  16% /
codelab-demo-controller:/apps        20G  5.2G   15G  26% /apps
codelab-demo-controller:/home        20G  5.2G   15G  26% /home
codelab-demo-controller:/etc/munge   20G  5.2G   15G  26% /etc/munge
tmpfs                               1.5G     0  1.5G   0% /run/user/3001</code></pre>
<p>Now, let us make sure we have a partition named &#34;first-partition&#34;.</p>
<pre><code>$ sinfo
PARTITION       AVAIL  TIMELIMIT  NODES  STATE NODELIST
first-partition    up   infinite      2  idle~ first-partition-[0-1]</code></pre>
<p>Notice that the compute instances in the first-partition currently have the name prefix &#34;first-partition&#34;. Use Slurm&#39;s &#34;scontrol&#34; command to verify the number of cores and amount of memory available for compute nodes.</p>
<pre><code>$ scontrol show node=first-partition-0
NodeName=partition-1-0 CoresPerSocket=1 
   CPUAlloc=0 CPUTot=2 CPULoad=N/A
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=(null)
   NodeAddr=first-partition-0 NodeHostName=first-partition-0 Port=0 
   RealMemory=7055 AllocMem=0 FreeMem=N/A Sockets=1 Boards=1
   State=IDLE+CLOUD+POWER ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=partition-1 
   BootTime=None SlurmdStartTime=None
   CfgTRES=cpu=2,mem=7055M,billing=2
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s</code></pre>
<h2 is-upgraded><strong>Submit a test job</strong></h2>
<p>To submit a job on this cluster, your POSIX user information needs to be registered in Slurm&#39;s accounting system. <code>cluster-services</code> acts as a wrapper for Slurm&#39;s <a href="https://slurm.schedmd.com/sacctmgr.html" target="_blank"><code>sacctmgr</code></a> command-line interface and intermediates your cluster description in the <a href="https://help.fluidnumerics.com/slurm-gcp/documentation/cluster-services-cli/cluster-configuration-schema" target="_blank">cluster-configuration file</a>. In this tutorial, we will cover adding users to a default Slurm account.</p>
<ol type="1" start="1">
<li>Add yourself to Slurm&#39;s accounting, using the following command and replacing [USERNAME] with your username.</li>
</ol>
<aside class="special"><p><strong>Tip: </strong>On Ubuntu fluid-slurm-gcp systems, you may need to reference the full path <code>/apps/cls/bin/cluster-services</code></p>
</aside>
<pre><code>$ sudo su
[root]# cluster-services add user [USERNAME]</code></pre>
<p>The output of this command should look similar to :</p>
<pre><code> Adding User(s)
  joe
 Associations =
  U = joe       A = default    C = codelab-de
 Non Default Settings</code></pre>
<aside class="special"><p><strong>Tip: </strong>OS Login provides a default username generated from your email address. For example, if your email address is developer@gmail.com, your POSIX username on Linux instances with OS Login enabled is developer_gmail_com. Learn how to customize your POSIX account information with the <a href="https://developers.google.com/admin-sdk/directory/v1/reference/users/update" target="_blank">Directory API</a>.</p>
</aside>
<ol type="1" start="2">
<li>After you have added your account, exit from root.<br></li>
</ol>
<pre><code>[root]# exit
exit
$ </code></pre>
<ol type="1" start="3">
<li>Submit a jobstep on one node that reports the compute node&#39;s hostname using the <code>srun</code> command. Because this deployment does not set any of the partitions as a default compute partition, the <code>--partition</code> flag must be specified.<br></li>
</ol>
<pre><code>$ srun -N1 --partition=first-partition hostname
first-partition-0</code></pre>
<p><br>This action will cause a compute node to be created in GCP. It can take anywhere from 1-2 minutes for the instance to be created and the job to start.You should see the name of the first compute node in this partition as output to the screen (<code>first-partition-0</code>)<br></p>
<aside class="special"><p><strong>Tip: </strong>Navigate to the Compute Engine &gt; VM Instances page under the Products and Services menu to watch the compute be created.</p>
</aside>
<p>Congratulations! You have just run a (simple) job step on your cluster. Compute nodes can be spun down manually or they can be left for Slurm to spin-down automatically.</p>
<pre><code>$ sudo su
[root]# scontrol update node=first-partition-0 state=power_down
[root]# exit
exit
$ </code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="0">
        <p>In this codelab, you created a cloud-native HPC cluster and submitted a simple job step with the Slurm job scheduler on Google Cloud Platform.</p>
<h2 is-upgraded><strong>What&#39;s next?</strong></h2>
<p><a href="http://codelabs.fluidnumerics.com/create-a-high-availability-compute-partition" target="_blank">Learn how to configure a high availability compute partition (multi-zone)</a></p>
<p><a href="http://codelabs.fluidnumerics.com/create-a-globally-scalable-compute-partition" target="_blank">Learn how to configure a globally scalable compute partition (multi-region)</a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSd7EN_ptBOJ9Eg2vH-Bs95j8pD2sjaTAwdcLoWDBVmF0fVEfg/viewform?usp=sf_link" target="_blank">Submit your feedback and request new codelabs using our feedback form</a></p>
<h2 is-upgraded><strong>Further reading</strong></h2>
<p><a href="https://cloud.google.com/compute/docs/instances/managing-instance-access" target="_blank">Learn how to configure OS-Login to ssh to your cluster with 3rd party ssh tools</a></p>
<p><a href="https://cloud.google.com/compute/docs/oslogin/manage-oslogin-in-an-org" target="_blank">Learn how to manage POSIX user information with the directory API</a></p>
<h2 is-upgraded><strong>Reference docs</strong></h2>
<p><a href="https://help.fluidnumerics.com/slurm-gcp" target="_blank">https://help.fluidnumerics.com/slurm-gcp</a></p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
