
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Create a Research Computing Cluster on Google Cloud</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="UA-125720260-6"
                  id="fluid-slurm-gcp/create-a-research-computing-cluster-on-google-cloud"
                  title="Create a Research Computing Cluster on Google Cloud"
                  environment="web"
                  feedback-link="https://forms.gle/q5HaN43HyVXLSLmM9">
    
      <google-codelab-step label="Introduction" duration="0">
        <p class="image-container"><img style="width: 130.00px" src="img/2a9f9a03df3ed213.png"></p>
<p><strong>Last Updated:</strong> 2022-07-01</p>
<h2 is-upgraded><strong>Use cases for  Fluid Numerics&#39; Research Computing Cluster on Google Cloud Platform</strong></h2>
<p>Organizations are beginning to investigate how to leverage cloud computing resources to meet highly variable high performance computing needs that often exceed the capacity of on-premise compute resources. </p>
<p>Google Cloud Platform (GCP) offers burstable access to a staggering amount of compute resources worldwide that includes <a href="https://cloud.google.com/compute/docs/cpu-platforms" target="_blank">CPUs</a>, <a href="https://cloud.google.com/compute/docs/gpus/" target="_blank">GPUs</a>, and TPUs. GCP also offers flexible <a href="https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type" target="_blank">custom system configurations</a>, where you can set a desired core-to-memory ratio for each compute instance. Additionally, GCP provides access to a range of data storage solutions that can attach to compute instances, including<a href="https://cloud.google.com/compute/docs/disks/" target="_blank"> HDD and SSD persistent disks</a>, high bandwidth <a href="https://cloud.google.com/compute/docs/disks/#localssds" target="_blank">local SSDs</a>, <a href="https://cloud.google.com/filestore/" target="_blank">NFS file-systems</a>, <a href="https://cloud.google.com/storage/" target="_blank">object storage</a>, and <a href="https://github.com/FluidNumerics/lustre-gcp_terraform" target="_blank">cloud-native Lustre file-systems</a> to the cloud.</p>
<p>Fluid Numerics has worked with the open-source community to help develop a globally scalable and highly customizable HPC cluster on Google Cloud Platform with the familiar look-and-feel of traditional on-premises HPC clusters. The Research Computing Cloud  is built around the <a href="https://slurm.schedmd.com/" target="_blank">Slurm job scheduler</a> and also provides a suite of compilers, MPI flavors, GPU programming libraries, and profilers and debuggers. The <a href="https://console.cloud.google.com/marketplace/browse?project=fluid-cluster-ops&q=rcc" target="_blank">RCC Solutions</a> are offered with CentOS 7, Ubuntu 20.04, Debian 10, and Rocky Linux 8 operating systems. Further, the <a href="https://github.com/fluidnumerics/rcc-apps" target="_blank">RCC-Apps ecosystem</a> enables you to rapidly build your own VM images on top of ours to make it easy to deploy your applications on the cloud.</p>
<h3 is-upgraded><strong>Infrastructure</strong></h3>
<p class="image-container"><img style="width: 624.00px" src="img/89804a92fc4963bc.png"></p>
<p>Fluid Numerics&#39; RCC provides a design on cloud resources that is similar in look-and-feel to on-premise HPC infrastructure. The deployment has a number of static login nodes, where users access the cluster, and a <em>controller</em> instance that hosts the Slurm job scheduler. Compute nodes are either static or ephemeral.</p>
<p>Static compute nodes persist as long as the deployment is active and remain ready to receive workloads from Slurm. Ephemeral compute nodes are created on-the-fly, as needed, to meet variable compute capacity demands. When ephemeral nodes become idle, Slurm automatically removes the compute resources from GCP so that you pay only for compute cycles as needed. To develop a better understanding of the costs on GCP for the RCC check out our <a href="https://help.fluidnumerics.com/slurm-gcp/pricing" target="_blank">pricing documentation</a>. </p>
<h3 is-upgraded><strong>Software tools</strong></h3>
<h4 is-upgraded>Cluster-Services</h4>
<p><a href="https://help.fluidnumerics.com/slurm-gcp/documentation/cluster-services" target="_blank">Cluster-Services</a>, a command-line tool developed by Fluid Numerics to manage compute partitions and available machines, Slurm accounting configurations and compute partition access for users, and network attached storage mounts. All of the images for RCC come with this command-line tool to make managing your globally scalable HPC cluster simpler and cost-effective.</p>
<h4 is-upgraded>Spack</h4>
<p><a href="https://spack.io/" target="_blank">Spack</a> is becoming a well accepted package manager for HPC applications. Spack supports a <a href="https://spack.readthedocs.io/en/latest/package_list.html" target="_blank">large number of HPC packages</a>, while managing all of the package dependencies for you.  Spack offers a straightforward process to <a href="https://spack-tutorial.readthedocs.io/en/latest/tutorial_packaging.html" target="_blank">integrate your HPC software into the Spack ecosystem</a> so that you can share your code with others more easily. To get started with using Spack for HPC package management, check out the <a href="https://spack-tutorial.readthedocs.io/en/latest/tutorial_basics.html#installing-packages" target="_blank">Spack package installation tutorial</a>.</p>
<h4 is-upgraded>Singularity</h4>
<p>HPC developers are beginning to develop strategies for creating portable HPC applications. Containerization has proven to be an excellent solution for the portability problem by allowing developers to package development environments alongside applications and &#34;shipping&#34; the containers to production systems. We opt to offer Singularity as our containerization platform of choice.</p>
<p><a href="https://sylabs.io/singularity/" target="_blank">Singularity</a> is a container platform that was developed specifically for HPC uses. To run HPC container images, users do not require privilege escalation. Further, <a href="https://sylabs.io/guides/3.3/user-guide/mpi.html" target="_blank">Singularity works naturally with MPI applications</a> that leverage resources across multiple compute nodes and even has flags that allow you to <a href="https://sylabs.io/guides/3.5/user-guide/gpu.html" target="_blank">leverage GPU and multi-GPU platforms</a>. </p>
<h2 is-upgraded><strong>What you will build</strong></h2>
<p>In this codelab, you are going to create a cloud-native HPC cluster on Google Cloud Platform and configure high-availability (multi-zone) and globally-scalable (multi-region) compute partitions. </p>
<aside class="warning"><p><strong>Note: </strong><a href="https://cloud.google.com/products/calculator/#id=5df12891-6c50-4d3f-97b9-c483439af101" target="_blank">This tutorial is estimated to cost $2.09 on Google Cloud Platform</a> (actual costs may vary)</p>
</aside>
<h2 is-upgraded><strong>What you will learn</strong></h2>
<ul>
<li>How to deploy a cloud-native research computing cluster with Terraform</li>
<li>How to set up basic Slurm accounting with Fluid Numerics&#39; cluster-services</li>
<li>How to submit a job step with Slurm</li>
</ul>
<h2 is-upgraded><strong>What you will need</strong></h2>
<ul>
<li><a href="https://gsuite.google.com/" target="_blank">GSuite</a>, <a href="https://cloud.google.com/identity" target="_blank">Cloud Identity</a>, or <a href="https://www.google.com/gmail/" target="_blank">Gmail Account</a></li>
<li><a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects" target="_blank">Google Cloud Platform Project with Billing enabled</a></li>
<li>Project owner role on your GCP Project</li>
<li><a href="https://cloud.google.com/compute/quotas" target="_blank">Sufficient Compute Engine Quota</a> in two GCP regions</li>
<li>Basic Command-Line Linux Experience</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Deploy the RCC using Terraform" duration="1">
        <p>Now you have verified your quota and permissions and are ready to launch the Fluid-Slurm-GCP HPC cluster on Google Cloud Platform. In these next few steps, you will create your cluster.</p>
<ol type="1" start="1">
<li>Open <a href="https://console.cloud.google.com/marketplace/details/fluid-cluster-ops/fluid-slurm-gcp" target="_blank">https://console.cloud.google.com/marketplace/details/fluid-cluster-ops/fluid-slurm-gcp</a> in a new tab. </li>
</ol>
<aside class="warning"><p><strong>Caution:</strong> Make sure that you are in the same GCP project that you configured in the previous section and using the correct GSuite, Cloud Identity, or Gmail account.</p>
</aside>
<ol type="1" start="2">
<li>Click &#34;Launch&#34;</li>
<li>Set the following parameters, while leaving the others as their default.</li>
</ol>
<table>
<tr><td colspan="1" rowspan="1"><p>Deployment name</p>
</td><td colspan="1" rowspan="1"><p>codelab-demo</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Zone</p>
</td><td colspan="1" rowspan="1"><p>us-west1-b</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Partition Name</p>
</td><td colspan="1" rowspan="1"><p>first-partition</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Static node count</p>
</td><td colspan="1" rowspan="1"><p>0</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Max node count</p>
</td><td colspan="1" rowspan="1"><p>2</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Slurm Default Compute Partition. Machine type</p>
</td><td colspan="1" rowspan="1"><p>n1-standard-2</p>
</td></tr>
</table>
<ol type="1" start="4">
<li>Click &#34;Deploy&#34; and wait for the cluster to be created.</li>
</ol>
<p>Congratulations! You have just created the a login node and controller on Google Cloud Platform. These instances can be used to spin-up ephemeral compute nodes to execute HPC workloads.</p>
<aside class="warning"><p><strong>Caution:</strong> Sometimes, the deployment-manager page will show the solution has been created, but the startup-scripts may still be configuring. If you log in to the login node and see that /home, /apps, and /etc have not been mounted, log out and wait for (at most) another minute.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Verify and test the Slurm Cluster" duration="10">
        <iframe class="youtube-video" src="https://www.youtube.com/embed/9fj7ne6cmH0?rel=0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>In this section, we will access the cluster&#39;s login node, verify mount and partition settings, add our account to the default Slurm account, and submit a simple job to verify that the cluster is working.</p>
<h2 is-upgraded>Verify your cluster has been properly set up</h2>
<p class="image-container"><img style="width: 624.00px" src="img/e00bfb83ad63227f.png"></p>
<p>Now that your deployment has been launched, it is time to access the login node. You can do this by clicking the SSH button from the deployment-manager page. This will open up a browser window with an active terminal on the login node.</p>
<p class="image-container"><img style="width: 514.50px" src="img/d95aab376f987b1d.png"></p>
<p>Once you have logged into the login node, let&#39;s make sure that /home, /apps, and /etc are mounted from the controller.</p>
<pre><code>$ df -h
Filesystem                          Size  Used Avail Use% Mounted on
devtmpfs                            7.3G     0  7.3G   0% /dev
tmpfs                               7.3G     0  7.3G   0% /dev/shm
tmpfs                               7.3G  8.5M  7.3G   1% /run
tmpfs                               7.3G     0  7.3G   0% /sys/fs/cgroup
/dev/sda1                            20G  3.1G   17G  16% /
codelab-demo-controller:/apps        20G  5.2G   15G  26% /apps
codelab-demo-controller:/home        20G  5.2G   15G  26% /home
codelab-demo-controller:/etc/munge   20G  5.2G   15G  26% /etc/munge
tmpfs                               1.5G     0  1.5G   0% /run/user/3001</code></pre>
<p>Now, let us make sure we have a partition named &#34;first-partition&#34;.</p>
<pre><code>$ sinfo
PARTITION       AVAIL  TIMELIMIT  NODES  STATE NODELIST
first-partition    up   infinite      2  idle~ first-partition-[0-1]</code></pre>
<p>Notice that the compute instances in the first-partition currently have the name prefix &#34;first-partition&#34;. Use Slurm&#39;s &#34;scontrol&#34; command to verify the number of cores and amount of memory available for compute nodes.</p>
<pre><code>$ scontrol show node=first-partition-0
NodeName=partition-1-0 CoresPerSocket=1 
   CPUAlloc=0 CPUTot=2 CPULoad=N/A
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=(null)
   NodeAddr=first-partition-0 NodeHostName=first-partition-0 Port=0 
   RealMemory=7055 AllocMem=0 FreeMem=N/A Sockets=1 Boards=1
   State=IDLE+CLOUD+POWER ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=partition-1 
   BootTime=None SlurmdStartTime=None
   CfgTRES=cpu=2,mem=7055M,billing=2
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s</code></pre>
<h2 is-upgraded><strong>Submit a test job</strong></h2>
<p>To submit a job on this cluster, your POSIX user information needs to be registered in Slurm&#39;s accounting system. <code>cluster-services</code> acts as a wrapper for Slurm&#39;s <a href="https://slurm.schedmd.com/sacctmgr.html" target="_blank"><code>sacctmgr</code></a> command-line interface and intermediates your cluster description in the <a href="https://help.fluidnumerics.com/slurm-gcp/documentation/cluster-services-cli/cluster-configuration-schema" target="_blank">cluster-configuration file</a>. In this tutorial, we will cover adding users to a default Slurm account.</p>
<ol type="1" start="1">
<li>Add yourself to Slurm&#39;s accounting, using the following command and replacing [USERNAME] with your username.</li>
</ol>
<aside class="special"><p><strong>Tip: </strong>On Ubuntu fluid-slurm-gcp systems, you may need to reference the full path <code>/apps/cls/bin/cluster-services</code></p>
</aside>
<pre><code>$ sudo su
[root]# cluster-services add user [USERNAME]</code></pre>
<p>The output of this command should look similar to :</p>
<pre><code> Adding User(s)
  joe
 Associations =
  U = joe       A = default    C = codelab-de
 Non Default Settings</code></pre>
<aside class="special"><p><strong>Tip: </strong>OS Login provides a default username generated from your email address. For example, if your email address is developer@gmail.com, your POSIX username on Linux instances with OS Login enabled is developer_gmail_com. Learn how to customize your POSIX account information with the <a href="https://developers.google.com/admin-sdk/directory/v1/reference/users/update" target="_blank">Directory API</a>.</p>
</aside>
<ol type="1" start="2">
<li>After you have added your account, exit from root.<br></li>
</ol>
<pre><code>[root]# exit
exit
$ </code></pre>
<ol type="1" start="3">
<li>Submit a jobstep on one node that reports the compute node&#39;s hostname using the <code>srun</code> command. Because this deployment does not set any of the partitions as a default compute partition, the <code>--partition</code> flag must be specified.<br></li>
</ol>
<pre><code>$ srun -N1 --partition=first-partition hostname
first-partition-0</code></pre>
<p><br>This action will cause a compute node to be created in GCP. It can take anywhere from 1-2 minutes for the instance to be created and the job to start.You should see the name of the first compute node in this partition as output to the screen (<code>first-partition-0</code>)<br></p>
<aside class="special"><p><strong>Tip: </strong>Navigate to the Compute Engine &gt; VM Instances page under the Products and Services menu to watch the compute be created.</p>
</aside>
<p>Congratulations! You have just run a (simple) job step on your cluster. Compute nodes can be spun down manually or they can be left for Slurm to spin-down automatically.</p>
<pre><code>$ sudo su
[root]# scontrol update node=first-partition-0 state=power_down
[root]# exit
exit
$ </code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="0">
        <p>In this codelab, you created a cloud-native HPC cluster and submitted a simple job step with the Slurm job scheduler on Google Cloud Platform.</p>
<h2 is-upgraded><strong>What&#39;s next?</strong></h2>
<p><a href="http://codelabs.fluidnumerics.com/create-a-high-availability-compute-partition" target="_blank">Learn how to configure a high availability compute partition (multi-zone)</a></p>
<p><a href="http://codelabs.fluidnumerics.com/create-a-globally-scalable-compute-partition" target="_blank">Learn how to configure a globally scalable compute partition (multi-region)</a></p>
<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSd7EN_ptBOJ9Eg2vH-Bs95j8pD2sjaTAwdcLoWDBVmF0fVEfg/viewform?usp=sf_link" target="_blank">Submit your feedback and request new codelabs using our feedback form</a></p>
<h2 is-upgraded><strong>Further reading</strong></h2>
<p><a href="https://cloud.google.com/compute/docs/instances/managing-instance-access" target="_blank">Learn how to configure OS-Login to ssh to your cluster with 3rd party ssh tools</a></p>
<p><a href="https://cloud.google.com/compute/docs/oslogin/manage-oslogin-in-an-org" target="_blank">Learn how to manage POSIX user information with the directory API</a></p>
<h2 is-upgraded><strong>Reference docs</strong></h2>
<p><a href="https://help.fluidnumerics.com/slurm-gcp" target="_blank">https://help.fluidnumerics.com/slurm-gcp</a></p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
